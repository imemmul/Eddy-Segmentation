{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/emir/anaconda3/envs/emirenv/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import SegformerFeatureExtractor\n",
    "import sys\n",
    "sys.path.insert(1, '../utils/')\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from create_dataloaders import EddyDatasetTrain, EddyDatasetValid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/emir/anaconda3/envs/emirenv/lib/python3.10/site-packages/transformers/models/segformer/image_processing_segformer.py:102: FutureWarning: The `reduce_labels` parameter is deprecated and will be removed in a future version. Please use `do_reduce_labels` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = SegformerFeatureExtractor.from_pretrained(\"nvidia/segformer-b0-finetuned-ade-512-512\")\n",
    "feature_extractor.reduce_labels = False\n",
    "feature_extractor.size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image_dir = \"/home/emir/dev/segmentation_eddies/downloads/data4test/aug_data/\"\n",
    "mask_image_dir = \"/home/emir/dev/segmentation_eddies/downloads/data4test/aug_label/\"\n",
    "val_image_dir = \"/home/emir/dev/segmentation_eddies/downloads/data4test/data/\"\n",
    "val_mask_dir = \"/home/emir/dev/segmentation_eddies/downloads/data4test/label/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_len = len(os.listdir(input_image_dir))\n",
    "split = int(0.85 * train_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = EddyDatasetTrain(feature_extractor=feature_extractor, input_image_dir=input_image_dir, mask_image_dir=mask_image_dir, split=split)\n",
    "valid_data = EddyDatasetValid(feature_extractor=feature_extractor, input_image_dir=val_image_dir, mask_image_dir=val_mask_dir, split=split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9180, 1620)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data), len(valid_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_train_data(data):\n",
    "    rand_ind = np.random.randint(len(train_data))\n",
    "    fig = plt.figure(figsize=(15, 10))\n",
    "    rows = 1\n",
    "    columns = 2\n",
    "    fig.add_subplot(rows, columns, 1)\n",
    "    plt.imshow((data[rand_ind]['pixel_values']).permute(1,2,0))\n",
    "    plt.axis(False)\n",
    "    plt.title(\"Image\")\n",
    "    fig.add_subplot(rows, columns, 2)\n",
    "    plt.imshow((data[rand_ind]['labels']))\n",
    "    plt.axis(False)\n",
    "    plt.title(\"Label\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 128])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]['labels'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x1000 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_train_data(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_data, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "287"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SegformerImageProcessor {\n",
       "  \"do_normalize\": true,\n",
       "  \"do_reduce_labels\": true,\n",
       "  \"do_rescale\": true,\n",
       "  \"do_resize\": true,\n",
       "  \"feature_extractor_type\": \"SegformerFeatureExtractor\",\n",
       "  \"image_mean\": [\n",
       "    0.485,\n",
       "    0.456,\n",
       "    0.406\n",
       "  ],\n",
       "  \"image_processor_type\": \"SegformerImageProcessor\",\n",
       "  \"image_std\": [\n",
       "    0.229,\n",
       "    0.224,\n",
       "    0.225\n",
       "  ],\n",
       "  \"reduce_labels\": false,\n",
       "  \"resample\": 2,\n",
       "  \"rescale_factor\": 0.00392156862745098,\n",
       "  \"size\": 128\n",
       "}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_weights = \"nvidia/segformer-b0-finetuned-ade-512-512\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from datasets import load_metric\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SegformerForSemanticSegmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 2.81k/2.81k [00:00<00:00, 1.57MB/s]\n",
      "Downloading: 100%|██████████| 70.0k/70.0k [00:00<00:00, 272kB/s] \n",
      "Downloading: 100%|██████████| 14.4M/14.4M [00:01<00:00, 8.04MB/s]\n",
      "Some weights of the model checkpoint at nvidia/mit-b0 were not used when initializing SegformerForSemanticSegmentation: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing SegformerForSemanticSegmentation from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing SegformerForSemanticSegmentation from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/mit-b0 and are newly initialized: ['decode_head.linear_c.1.proj.weight', 'decode_head.batch_norm.num_batches_tracked', 'decode_head.linear_fuse.weight', 'decode_head.linear_c.3.proj.bias', 'decode_head.linear_c.2.proj.bias', 'decode_head.batch_norm.weight', 'decode_head.linear_c.0.proj.weight', 'decode_head.linear_c.1.proj.bias', 'decode_head.batch_norm.bias', 'decode_head.classifier.weight', 'decode_head.batch_norm.running_mean', 'decode_head.linear_c.3.proj.weight', 'decode_head.linear_c.2.proj.weight', 'decode_head.batch_norm.running_var', 'decode_head.classifier.bias', 'decode_head.linear_c.0.proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import SegformerForSemanticSegmentation\n",
    "import json\n",
    "from huggingface_hub import cached_download, hf_hub_url\n",
    "\n",
    "# load id2label mapping from a JSON on the hub\n",
    "repo_id = \"huggingface/label-files\"\n",
    "filename = \"ade20k-id2label.json\"\n",
    "id2label = json.load(open(cached_download(hf_hub_url(repo_id, filename, repo_type='dataset')), \"r\"))\n",
    "id2label = {int(k): v for k, v in id2label.items()}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "# define model\n",
    "model = SegformerForSemanticSegmentation.from_pretrained(\"nvidia/mit-b0\",\n",
    "                                                         num_labels=150, \n",
    "                                                         id2label=id2label, \n",
    "                                                         label2id=label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"mean_iou\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/emir/.cache/huggingface/modules/datasets_modules/metrics/mean_iou/d4add40cf977cdd73590b5873fa830f3f13adb678f6777a29fb07b7c81d14342/mean_iou.py:258: RuntimeWarning: invalid value encountered in divide\n",
      "  iou = total_area_intersect / total_area_union\n",
      "/home/emir/.cache/huggingface/modules/datasets_modules/metrics/mean_iou/d4add40cf977cdd73590b5873fa830f3f13adb678f6777a29fb07b7c81d14342/mean_iou.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  acc = total_area_intersect / total_area_label\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 4.839908599853516\n",
      "Mean_iou: 0.00016161606263116767\n",
      "Mean accuracy: 0.010349990056633326\n",
      "Loss: 3.197761297225952\n",
      "Mean_iou: 0.0557980590470053\n",
      "Mean accuracy: 0.7792054167205551\n",
      "Loss: 1.9932997226715088\n",
      "Mean_iou: 0.2858375564808675\n",
      "Mean accuracy: 0.6705571411845552\n",
      "Epoch: 1\n",
      "Loss: 1.2289550304412842\n",
      "Mean_iou: 0.5323069789218681\n",
      "Mean accuracy: 0.5417970521630587\n",
      "Loss: 0.6507930755615234\n",
      "Mean_iou: 0.6164517145194925\n",
      "Mean accuracy: 0.6293937027152363\n",
      "Loss: 0.33203214406967163\n",
      "Mean_iou: 0.5189236989339944\n",
      "Mean accuracy: 0.5268034351976031\n",
      "Epoch: 2\n",
      "Loss: 0.2615724205970764\n",
      "Mean_iou: 0.6318491863519304\n",
      "Mean accuracy: 0.657173971400869\n",
      "Loss: 0.14938677847385406\n",
      "Mean_iou: 0.5810744545511276\n",
      "Mean accuracy: 0.5941196178196315\n",
      "Loss: 0.13393421471118927\n",
      "Mean_iou: 0.5929559184113665\n",
      "Mean accuracy: 0.6110315522523162\n",
      "Epoch: 3\n",
      "Loss: 0.10107080638408661\n",
      "Mean_iou: 0.5488479123697381\n",
      "Mean accuracy: 0.5559729391894487\n",
      "Loss: 0.10176530480384827\n",
      "Mean_iou: 0.6012496618679041\n",
      "Mean accuracy: 0.6408588787390357\n",
      "Loss: 0.08522547781467438\n",
      "Mean_iou: 0.6146267202947742\n",
      "Mean accuracy: 0.6313697065772566\n",
      "Epoch: 4\n",
      "Loss: 0.08403722941875458\n",
      "Mean_iou: 0.6507802947710122\n",
      "Mean accuracy: 0.6833268708287461\n",
      "Loss: 0.0628807470202446\n",
      "Mean_iou: 0.6141050723898736\n",
      "Mean accuracy: 0.625254092716465\n",
      "Loss: 0.06666013598442078\n",
      "Mean_iou: 0.6186996554583336\n",
      "Mean accuracy: 0.6330275683694924\n",
      "Epoch: 5\n",
      "Loss: 0.05656563118100166\n",
      "Mean_iou: 0.6710956819158497\n",
      "Mean accuracy: 0.7064201489192671\n",
      "Loss: 0.05549139529466629\n",
      "Mean_iou: 0.5803505806400012\n",
      "Mean accuracy: 0.5945109634123602\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.00006)\n",
    "# move model to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model.to(device)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(50):  # loop over the dataset multiple times\n",
    "   print(\"Epoch:\", epoch)\n",
    "   for idx, batch in enumerate((train_dataloader)):\n",
    "        # get the inputs;\n",
    "        pixel_values = batch[\"pixel_values\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "        loss, logits = outputs.loss, outputs.logits\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # evaluate\n",
    "        with torch.no_grad():\n",
    "          upsampled_logits = nn.functional.interpolate(logits, size=labels.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "          predicted = upsampled_logits.argmax(dim=1)\n",
    "          \n",
    "          # note that the metric expects predictions + labels as numpy arrays\n",
    "          metric.add_batch(predictions=predicted.detach().cpu().numpy(), references=labels.detach().cpu().numpy())\n",
    "\n",
    "        # let's print loss and metrics every 100 batches\n",
    "        if idx % 100 == 0:\n",
    "          metrics = metric._compute(num_labels=len(id2label), \n",
    "                                   ignore_index=255,\n",
    "                                   reduce_labels=False,\n",
    "                                   references=labels.cpu().numpy(),\n",
    "                                   predictions=predicted.cpu().numpy()\n",
    "                                    # we've already reduced the labels before)\n",
    "          )\n",
    "\n",
    "          print(\"Loss:\", loss.item())\n",
    "          print(\"Mean_iou:\", metrics[\"mean_iou\"])\n",
    "          print(\"Mean accuracy:\", metrics[\"mean_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('emirenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "12387375b9bdc6a02744d740ba1c0f3a29dccca8b0d19fb734bd586653bcb972"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
